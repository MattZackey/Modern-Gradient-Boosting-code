{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bb43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, plot_confusion_matrix, f1_score, accuracy_score, matthews_corrcoef\n",
    "from xgboost import cv\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560a82a",
   "metadata": {},
   "source": [
    "# XGBoost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e262db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform cross-validation with XGBoost for a grid of parameters.\n",
    "def xgboost_tune_class(param_comb, X, y, num_iterations, row_subsample, colsample, tree_build, class_imbal, sparse, learning_rate, cv_metric):\n",
    "\n",
    "    param_comb = np.hstack((param_comb, np.zeros((param_comb.shape[0], 2))))\n",
    "\n",
    "    #Checking if sparse matrix. If so, then 0 entries are treated as missing.\n",
    "    if(sparse == True): \n",
    "        xgb_dat = xgb.DMatrix(data=X,label=y,missing=0)\n",
    "    else: \n",
    "        xgb_dat = xgb.DMatrix(data=X,label=y)\n",
    "    \n",
    "    #Checking if class imbalance\n",
    "    if(class_imbal == True): \n",
    "        weight_pos = sum(y==0)/sum(y==1)\n",
    "    else: \n",
    "        weight_pos = 1\n",
    "\n",
    "    for i in range(param_comb.shape[0]):\n",
    "  \n",
    "        #Setting up parameters.\n",
    "        xgb_params = {\n",
    "            'eta' : learning_rate,\n",
    "            'gamma' : param_comb[i, 0],\n",
    "            'max_depth' : int(param_comb[i, 1]),\n",
    "            'subsample' : row_subsample,\n",
    "            'colsample_bytree' : colsample,\n",
    "            'reg_alpha': param_comb[i, 2],\n",
    "            'reg_lambda' : param_comb[i, 3],\n",
    "            'tree_method' : tree_build,\n",
    "            'scale_pos_weight' : weight_pos,\n",
    "            'objective' : 'binary:logistic',\n",
    "            'n_jobs' : 1\n",
    "        }\n",
    "\n",
    "        #Performing cross-validation\n",
    "        xgb_cv = xgb.cv(params = xgb_params,\n",
    "                        dtrain = xgb_dat,\n",
    "                        num_boost_round = num_iterations,\n",
    "                        nfold = 5,\n",
    "                        stratified = True,\n",
    "                        metrics = cv_metric,\n",
    "                        early_stopping_rounds = 200,\n",
    "                        seed = 75,\n",
    "                        verbose_eval = 100)\n",
    "    \n",
    "        if(cv_metric == 'auc'):\n",
    "            num_tree = np.argmax(xgb_cv.iloc[:,2]) + 1 #number of trees\n",
    "            opt_val = max(xgb_cv.iloc[:,2])\n",
    "        else:\n",
    "            num_tree = np.argmin(xgb_cv.iloc[:,2]) + 1 #number of trees\n",
    "            opt_val = min(xgb_cv.iloc[:,2])\n",
    "\n",
    "        param_comb[i,4:6] = num_tree, opt_val\n",
    "        \n",
    "        print(i)\n",
    "    \n",
    "    return(param_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1271c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to fit an XGBoost model with the optimal set of parameter values determined from cross-validation.\n",
    "def xgboost_fit_class(param_opt, X, y, row_subsample, colsample, tree_build, class_imbal, sparse, learning_rate):\n",
    "\n",
    "    #Checking if sparse matrix. If so, then 0 entries are treated as missing.\n",
    "    if(sparse == True): \n",
    "        missing_val = 0\n",
    "    else: \n",
    "        missing_val = np.nan\n",
    "    \n",
    "    #Checking if user specified class imbalance.\n",
    "    if(class_imbal == True): \n",
    "        weight_pos = sum(y==0)/sum(y==1)\n",
    "    else: \n",
    "        weight_pos = 1\n",
    "    \n",
    "    clf_xgb = xgb.XGBClassifier(eta = learning_rate,\n",
    "                                gamma = param_opt[0],\n",
    "                                max_depth = int(param_opt[1]),\n",
    "                                subsample = row_subsample,\n",
    "                                colsample_bytree = colsample,\n",
    "                                reg_alpha = param_opt[2],\n",
    "                                reg_lambda = param_opt[3],\n",
    "                                tree_method = tree_build,\n",
    "                                scale_pos_weight = weight_pos,\n",
    "                                objective = 'binary:logistic',\n",
    "                                seed = 33,\n",
    "                                n_estimators = int(param_opt[4]),\n",
    "                                missing = missing_val,\n",
    "                                verbosity = 1,\n",
    "                                n_jobs = 1)\n",
    "    #Fitting model\n",
    "    clf_xgb.fit(X, y)\n",
    "    \n",
    "    return(clf_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1fb4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove all the no regurlisation parameters from the parameter grid.\n",
    "def regularisation_param(gamma, max_depth, l1, l2):\n",
    "\n",
    "    param_comb_reg = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "    param_comb_reg = np.array(param_comb_reg)\n",
    "\n",
    "    #Removing no regularisation in grid (where gamma == 0 & l1 == 0 & l2 == 0)\n",
    "    zero_gamma = param_comb_reg[:,0] == 0\n",
    "    zero_alpha = param_comb_reg[:,2] == 0\n",
    "    zero_lambda = param_comb_reg[:,3] == 0\n",
    "    param_comb_reg = param_comb_reg[~zero_gamma | ~zero_alpha | ~zero_lambda,:]\n",
    "    \n",
    "    return(np.array(param_comb_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6109ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the errors of the models.\n",
    "def errors_model(mod, X_test, y_test):\n",
    "    \n",
    "    pred_class = mod.predict(X_test) #Class predictions\n",
    "    acc = accuracy_score(y_test, pred_class) #Accuracy\n",
    "    f1 = f1_score(y_test, pred_class) #F1 score \n",
    "    matt = matthews_corrcoef(y_test, pred_class) #Matthews Correlation Coefficient\n",
    "    pred_prob = mod.predict_proba(X_test)[:,1] #Probability predictions\n",
    "    auc = roc_auc_score(y_test, pred_prob) #AUC\n",
    "    errs_mod = [acc, f1, matt, auc]\n",
    "    return(errs_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No scientific notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19ff8a",
   "metadata": {},
   "source": [
    "# AdaBoost (Overlap data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a50eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset and setting up data \n",
    "import pickle\n",
    "file_name = 'Overlap_data.pickle'\n",
    "file = open(file_name,'rb')\n",
    "Overlap_data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train_over = Overlap_data['X_train']\n",
    "y_train_over = Overlap_data['y_train']\n",
    "X_test_over = Overlap_data['X_test']\n",
    "y_test_over = Overlap_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af375b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid no regularisation.\n",
    "gamma = [0]\n",
    "max_depth = list(range(1, 11))\n",
    "l1 = [0]\n",
    "l2 = [0]\n",
    "param_noreg_over = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "param_noreg_over = np.array(param_noreg_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with no regularisation.\n",
    "cv_noreg_over = xgboost_tune_class(param_comb = param_noreg_over, \n",
    "                                   X = X_train_over, \n",
    "                                   y = y_train_over, \n",
    "                                   learning_rate = 0.05, \n",
    "                                   num_iterations = 10000, \n",
    "                                   row_subsample = 1, \n",
    "                                   colsample = 1, \n",
    "                                   tree_build = 'exact', \n",
    "                                   class_imbal = False, \n",
    "                                   sparse = False,\n",
    "                                   cv_metric = 'logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2168e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with no regularisation.\n",
    "opt_ind_noreg_over = np.argmin(cv_noreg_over[:,5])\n",
    "xgmod_noreg_over = xgboost_fit_class(param_opt = cv_noreg_over[opt_ind_noreg_over,:], \n",
    "                                     X = X_train_over, \n",
    "                                     y = y_train_over, \n",
    "                                     learning_rate = 0.05,\n",
    "                                     row_subsample = 1, \n",
    "                                     colsample = 1, \n",
    "                                     tree_build = 'exact',\n",
    "                                     class_imbal = False,\n",
    "                                     sparse = False)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_noreg_over = errors_model(mod = xgmod_noreg_over, X_test = X_test_over, y_test = y_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59a038c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid regularisation.\n",
    "\n",
    "#Parameters 1\n",
    "###################################################################################\n",
    "gamma = [0, 0.5, 1, 2, 3]\n",
    "max_depth = [1, 2, 3]\n",
    "l1 = [0, 0.5, 1, 2, 3]\n",
    "l2 = [0, 0.5, 1, 2, 3]\n",
    "param_reg_over = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################\n",
    "\n",
    "#Parameters 2 (refined)\n",
    "###################################################################################\n",
    "#gamma = [0]\n",
    "#max_depth = [1]\n",
    "#l1 = [0.5]\n",
    "#l2 = [3, 3.5, 4, 4.5 ,5]\n",
    "#param_reg_over = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a34054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with regularisation.\n",
    "cv_reg_over = xgboost_tune_class(param_comb = param_reg_over,\n",
    "                                 X = X_train_over, \n",
    "                                 y = y_train_over, \n",
    "                                 learning_rate = 0.05, \n",
    "                                 num_iterations = 10000, \n",
    "                                 row_subsample = 1, \n",
    "                                 colsample = 1, \n",
    "                                 tree_build = 'exact', \n",
    "                                 class_imbal = False, \n",
    "                                 sparse = False,\n",
    "                                 cv_metric = 'logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b435760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with regularisation.\n",
    "opt_ind_reg_over = np.argmin(cv_reg_over[:,5])\n",
    "xgmod_reg_over = xgboost_fit_class(param_opt = cv_reg_over[opt_ind_reg_over,:], \n",
    "                                     X = X_train_over, \n",
    "                                     y = y_train_over, \n",
    "                                     learning_rate = 0.05,\n",
    "                                     row_subsample = 1, \n",
    "                                     colsample = 1, \n",
    "                                     tree_build = 'exact',\n",
    "                                     class_imbal = False,\n",
    "                                     sparse = False)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_reg_over = errors_model(mod = xgmod_reg_over, X_test = X_test_over, y_test = y_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table of parameters and test errors. \n",
    "test_err_noreg = np.append(cv_noreg_over[opt_ind_noreg_over,0:5],errs_noreg_over)\n",
    "test_err_reg = np.append(cv_reg_over[opt_ind_reg_over,0:5],errs_reg_over)\n",
    "test_err_over = np.row_stack((test_err_noreg,test_err_reg))\n",
    "np.round(test_err_over,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec818e",
   "metadata": {},
   "source": [
    "# Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7124c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and setting up data.\n",
    "file_path_ph = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\phoneme.csv'\n",
    "df_ph = pd.read_csv(file_path_ph)\n",
    "X_ph = df_ph.drop('target',axis=1).copy()\n",
    "y_ph = df_ph['target'].copy()\n",
    "X_train_ph, X_test_ph, y_train_ph, y_test_ph = train_test_split(X_ph, y_ph, random_state=65, stratify=y_ph, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6aa797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid no regularisation.\n",
    "gamma = [0]\n",
    "max_depth = list(range(1, 11))\n",
    "l1 = [0]\n",
    "l2 = [0]\n",
    "param_noreg_ph = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "param_noreg_ph = np.array(param_noreg_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1db08288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with no regularisation.\n",
    "cv_noreg_ph = xgboost_tune_class(param_comb = param_noreg_ph, \n",
    "                                 X = X_train_ph, \n",
    "                                 y = y_train_ph, \n",
    "                                 learning_rate = 0.05, \n",
    "                                 num_iterations = 10000, \n",
    "                                 row_subsample = 0.8, \n",
    "                                 colsample = 1, \n",
    "                                 tree_build = 'exact', \n",
    "                                 class_imbal = True, \n",
    "                                 sparse = False,\n",
    "                                 cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa19e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with no regularisation.\n",
    "opt_ind_noreg_ph = np.argmax(cv_noreg_ph[:,5])\n",
    "xgmod_noreg_ph = xgboost_fit_class(param_opt = cv_noreg_ph[opt_ind_noreg_ph,:], \n",
    "                                   X = X_train_ph, \n",
    "                                   y = y_train_ph, \n",
    "                                   learning_rate = 0.05,\n",
    "                                   row_subsample = 0.8, \n",
    "                                   colsample = 1, \n",
    "                                   tree_build = 'exact',\n",
    "                                   class_imbal = True,\n",
    "                                   sparse = False)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_noreg_ph = errors_model(mod = xgmod_noreg_ph, X_test = X_test_ph, y_test = y_test_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68040b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid regularisation.\n",
    "\n",
    "#Parameters 1\n",
    "#####################################################################\n",
    "gamma = [0, 0.5, 1, 2, 3]\n",
    "max_depth = [10]\n",
    "l1 = [0, 0.5, 1, 2, 3]\n",
    "l2 = [0, 0.5, 1, 2, 3]\n",
    "param_reg_ph = regularisation_param(gamma, max_depth, l1, l2)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d29a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with regularisation.\n",
    "cv_reg_ph = xgboost_tune_class(param_comb = param_reg_ph,\n",
    "                               X = X_train_ph, \n",
    "                               y = y_train_ph, \n",
    "                               learning_rate = 0.05, \n",
    "                               num_iterations = 10000, \n",
    "                               row_subsample = 0.8, \n",
    "                               colsample = 1, \n",
    "                               tree_build = 'exact', \n",
    "                               class_imbal = True, \n",
    "                               sparse = False,\n",
    "                               cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b381d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with regularisation.\n",
    "opt_ind_reg_ph = np.argmax(cv_reg_ph[:,5])\n",
    "xgmod_reg_ph = xgboost_fit_class(param_opt = cv_reg_ph[opt_ind_reg_ph,:], \n",
    "                                 X = X_train_ph, \n",
    "                                 y = y_train_ph, \n",
    "                                 learning_rate = 0.05,\n",
    "                                 row_subsample = 0.8, \n",
    "                                 colsample = 1, \n",
    "                                 tree_build = 'exact',\n",
    "                                 class_imbal = True,\n",
    "                                 sparse = False)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_reg_ph = errors_model(mod = xgmod_reg_ph, X_test = X_test_ph, y_test = y_test_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table of parameters and test errors. \n",
    "test_err_noreg = np.append(cv_noreg_ph[opt_ind_noreg_ph,0:5],errs_noreg_ph)\n",
    "test_err_reg = np.append(cv_reg_ph[opt_ind_reg_ph,0:5],errs_reg_ph)\n",
    "test_err_ph = np.row_stack((test_err_noreg,test_err_reg))\n",
    "np.round(test_err_ph,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca7bce",
   "metadata": {},
   "source": [
    "# Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1bcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and setting up data.\n",
    "file_path = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\adult1.csv'\n",
    "df_adult1 = pd.read_csv(file_path,header=None)\n",
    "file_path = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\adult2.csv'\n",
    "df_adult2 = pd.read_csv(file_path,header=None)\n",
    "df_adult = pd.concat([df_adult1, df_adult2])\n",
    "df_adult.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"martial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"target\"]\n",
    "df_adult.replace(' ','',regex=True,inplace=True)\n",
    "df_adult['target'] = df_adult['target'].apply(lambda x: x.rstrip('.'))\n",
    "df_adult['target'] = df_adult['target'].apply(lambda x: 0 if x==\"<=50K\" else 1)\n",
    "df_adult = df_adult.reset_index(drop=True)\n",
    "df_adult = df_adult.drop('education',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad39e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dummy varaibles and Training/Test split\n",
    "X_adult = df_adult.drop('target', axis = 1).copy()\n",
    "X_adult = pd.get_dummies(X_adult, columns = X_adult.columns[X_adult.dtypes==object], drop_first=True)\n",
    "y_adult = df_adult['target'].copy()\n",
    "X_train_adult, X_test_adult, y_train_adult, y_test_adult = train_test_split(X_adult, y_adult, random_state=65, stratify=y_adult, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceff603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid no regularisation.\n",
    "gamma = [0]\n",
    "max_depth = list(range(1, 11))\n",
    "l1 = [0]\n",
    "l2 = [0]\n",
    "param_noreg_adult = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "param_noreg_adult = np.array(param_noreg_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe45308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with no regularisation.\n",
    "cv_noreg_adult = xgboost_tune_class(param_comb = param_noreg_adult, \n",
    "                                    X = X_train_adult, \n",
    "                                    y = y_train_adult, \n",
    "                                    learning_rate = 0.05, \n",
    "                                    num_iterations = 10000, \n",
    "                                    row_subsample = 0.5, \n",
    "                                    colsample = 0.5, \n",
    "                                    tree_build = 'approx',\n",
    "                                    class_imbal = True,\n",
    "                                    sparse = True,\n",
    "                                    cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee60d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with no regularisation.\n",
    "opt_ind_noreg_adult = np.argmax(cv_noreg_adult[:,5])\n",
    "xgmod_noreg_adult = xgboost_fit_class(param_opt = cv_noreg_adult[opt_ind_noreg_adult,:], \n",
    "                                   X = X_train_adult, \n",
    "                                   y = y_train_adult, \n",
    "                                   learning_rate = 0.05,\n",
    "                                   row_subsample = 0.5, \n",
    "                                   colsample = 0.5, \n",
    "                                   tree_build = 'approx',\n",
    "                                   class_imbal = True,\n",
    "                                   sparse = True)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_noreg_adult = errors_model(mod = xgmod_noreg_adult, X_test = X_test_adult, y_test = y_test_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid regularisation.\n",
    "\n",
    "#Parameters 1\n",
    "###################################################################################\n",
    "gamma = [0, 0.5, 1, 2, 3]\n",
    "max_depth = [1, 2, 3]\n",
    "l1 = [0, 0.5, 1, 2, 3]\n",
    "l2 = [0, 0.5, 1, 2, 3]\n",
    "param_reg_adult = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c4b9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with regularisation.\n",
    "cv_reg_adult = xgboost_tune_class(param_comb = param_reg_adult,\n",
    "                                  X = X_train_adult, \n",
    "                                  y = y_train_adult, \n",
    "                                  learning_rate = 0.05, \n",
    "                                  num_iterations = 10000, \n",
    "                                  row_subsample = 0.5, \n",
    "                                  colsample = 0.5, \n",
    "                                  tree_build = 'approx',\n",
    "                                  class_imbal = True,\n",
    "                                  sparse = True,\n",
    "                                  cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d09069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with regularisation.\n",
    "opt_ind_reg_adult = np.argmax(cv_reg_adult[:,5])\n",
    "xgmod_reg_adult = xgboost_fit_class(param_opt = cv_reg_adult[opt_ind_reg_adult,:], \n",
    "                                   X = X_train_adult, \n",
    "                                   y = y_train_adult, \n",
    "                                   learning_rate = 0.05,\n",
    "                                   row_subsample = 0.5, \n",
    "                                   colsample = 0.5, \n",
    "                                   tree_build = 'approx',\n",
    "                                   class_imbal = True,\n",
    "                                   sparse = True)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_reg_adult = errors_model(mod = xgmod_reg_adult, X_test = X_test_adult, y_test = y_test_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4910aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table of parameters and test errors. \n",
    "test_err_noreg = np.append(cv_noreg_adult[opt_ind_noreg_adult,0:5],errs_noreg_adult)\n",
    "test_err_reg = np.append(cv_reg_adult[opt_ind_reg_adult,0:5],errs_reg_adult)\n",
    "test_err_adult = np.row_stack((test_err_noreg,test_err_reg))\n",
    "np.round(test_err_adult,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09de9de",
   "metadata": {},
   "source": [
    "# Santander Customer Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7011368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset and setting up data.\n",
    "import pickle\n",
    "file_name = 'customersatformat.pickle'\n",
    "file = open(file_name,'rb')\n",
    "df_sat = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training/Test split\n",
    "X_sat = df_sat.drop('TARGET', axis = 1).copy()\n",
    "y_sat = df_sat['TARGET'].copy()\n",
    "X_train_sat, X_test_sat, y_train_sat, y_test_sat = train_test_split(X_sat, y_sat, random_state=71, stratify=y_sat, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00bad5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid no regularisation.\n",
    "gamma = [0]\n",
    "max_depth = list(range(1, 11))\n",
    "l1 = [0]\n",
    "l2 = [0]\n",
    "param_noreg_sat = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "param_noreg_sat = np.array(param_noreg_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with no regularisation.\n",
    "cv_noreg_sat = xgboost_tune_class(param_comb = param_noreg_sat, \n",
    "                                  X = X_train_sat, \n",
    "                                  y = y_train_sat, \n",
    "                                  learning_rate = 0.05, \n",
    "                                  num_iterations = 10000, \n",
    "                                  row_subsample = 0.3, \n",
    "                                  colsample = 0.3, \n",
    "                                  tree_build = 'approx', \n",
    "                                  class_imbal = True, \n",
    "                                  sparse = True,\n",
    "                                  cv_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34593fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with no regularisation.\n",
    "opt_ind_noreg_sat = np.argmax(cv_noreg_sat[:,5])\n",
    "xgmod_noreg_sat = xgboost_fit_class(param_opt = cv_noreg_sat[opt_ind_noreg_sat,:], \n",
    "                                   X = X_train_sat, \n",
    "                                   y = y_train_sat, \n",
    "                                   learning_rate = 0.05,\n",
    "                                   row_subsample = 0.3, \n",
    "                                   colsample = 0.3, \n",
    "                                   tree_build = 'approx', \n",
    "                                   class_imbal = True, \n",
    "                                   sparse = True)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_noreg_sat = errors_model(mod = xgmod_noreg_sat, X_test = X_test_sat, y_test = y_test_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27692a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid regularisation\n",
    "\n",
    "#Parameters 1\n",
    "###################################################################################\n",
    "gamma = [0, 0.5, 1, 2, 3]\n",
    "max_depth = [2, 3, 4]\n",
    "l1 = [0, 0.5, 1, 2, 3]\n",
    "l2 = [0, 0.5, 1, 2, 3]\n",
    "param_reg_sat = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################\n",
    "\n",
    "#Parameters 2\n",
    "###################################################################################\n",
    "#gamma = [2]\n",
    "#max_depth = [3]\n",
    "#l1 = [2]\n",
    "#l2 = [3, 3.5, 4, 4.5 ,5]\n",
    "param_reg_sat = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b81e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with regularisation.\n",
    "cv_reg_sat = xgboost_tune_class(param_comb = param_reg_sat,\n",
    "                                X = X_train_sat, \n",
    "                                y = y_train_sat, \n",
    "                                learning_rate = 0.05, \n",
    "                                num_iterations = 10000, \n",
    "                                row_subsample = 0.3, \n",
    "                                colsample = 0.3, \n",
    "                                tree_build = 'approx', \n",
    "                                class_imbal = True, \n",
    "                                sparse = True,\n",
    "                                cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902d64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with regularisation.\n",
    "opt_ind_reg_sat = np.argmax(cv_reg_sat[:,5])\n",
    "xgmod_reg_sat = xgboost_fit_class(param_opt = cv_reg_sat[opt_ind_reg_sat,:], \n",
    "                                  X = X_train_sat, \n",
    "                                  y = y_train_sat, \n",
    "                                  learning_rate = 0.05,\n",
    "                                  row_subsample = 0.3, \n",
    "                                  colsample = 0.3, \n",
    "                                  tree_build = 'approx', \n",
    "                                  class_imbal = True, \n",
    "                                  sparse = True)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_reg_sat = errors_model(mod = xgmod_reg_sat, X_test = X_test_sat, y_test = y_test_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table of parameters and test errors. \n",
    "test_err_noreg = np.append(cv_noreg_sat[opt_ind_noreg_sat,0:5],errs_noreg_sat)\n",
    "test_err_reg = np.append(cv_reg_sat[opt_ind_reg_sat,0:5],errs_reg_sat)\n",
    "test_err_sat = np.row_stack((test_err_noreg,test_err_reg))\n",
    "np.round(test_err_sat,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
