{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e7dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, plot_confusion_matrix, f1_score, accuracy_score, matthews_corrcoef\n",
    "from xgboost import cv\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6929f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for performing cross-validation with LightGBM model\n",
    "def lightgbm_tune_class(param_comb, subsample_type, X, y, learning_rate, num_iterations, colsample, max_depth, class_imbal, efb_enable, cv_metric):\n",
    "\n",
    "    p = param_comb.shape[1]\n",
    "    param_comb = np.hstack((param_comb, np.zeros((param_comb.shape[0], 2))))\n",
    "\n",
    "    #Creating lightgbm dataset\n",
    "    lgb_dat = lgb.Dataset(data = X, label = y)\n",
    "    \n",
    "    #Checking if class imbalance\n",
    "    if(class_imbal == True): \n",
    "        weight_pos = sum(y==0)/sum(y==1)\n",
    "    else: \n",
    "        weight_pos = 1\n",
    "        \n",
    "    #Parameters\n",
    "    lgb_params = {\n",
    "            'boosting_type' : subsample_type,\n",
    "            'learning_rate' : learning_rate,\n",
    "            'max_depth' : max_depth,\n",
    "            'feature_fraction' : colsample,\n",
    "            'num_leaves' : 1100, #High so number of leaves does not affect tree building\n",
    "            'scale_pos_weight' : weight_pos,\n",
    "            'objective' : 'binary',\n",
    "            'enable_bundle' : efb_enable,\n",
    "            'n_jobs' : 1,\n",
    "            'verbose' : -1\n",
    "        }\n",
    "\n",
    "    for i in range(param_comb.shape[0]):\n",
    "  \n",
    "        #Setting up other parameters depending on subsample type\n",
    "        if(subsample_type == 'goss'):\n",
    "            lgb_params.update({'top_rate' : param_comb[i, 0], 'other_rate' : param_comb[i, 1]})\n",
    "        elif(subsample_type == 'gbdt'):\n",
    "            lgb_params.update({'bagging_fraction' : param_comb[i, 0],'bagging_freq' : 1})\n",
    "        else: \n",
    "            raise ValueError(\"Must give subsampling type\")\n",
    "        \n",
    "        lgb_cv = lgb.cv(params = lgb_params,\n",
    "                        train_set = lgb_dat,\n",
    "                        num_boost_round = num_iterations,\n",
    "                        nfold = 5,\n",
    "                        stratified = True,\n",
    "                        metrics = cv_metric,\n",
    "                        seed = 86)\n",
    "        \n",
    "        if(cv_metric == 'AUC'):\n",
    "            num_tree = np.argmax(lgb_cv.get('auc-mean')) + 1 #number of trees\n",
    "            opt_val = max(lgb_cv.get('auc-mean'))\n",
    "        else:\n",
    "            num_tree = np.argmin(lgb_cv.get('binary_logloss-mean')) + 1 #number of trees\n",
    "            opt_val = min(lgb_cv.get('binary_logloss-mean'))\n",
    "\n",
    "        param_comb[i,p:(p+2)] = num_tree, opt_val\n",
    "        \n",
    "        print(i)\n",
    "    \n",
    "    return(param_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2f5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for fitting LightGBM model\n",
    "def lightgbm_fit_class(param_vals, subsample_type, X, y, learning_rate, colsample, max_depth, class_imbal, efb_enable):\n",
    "    \n",
    "    #Checking if user specified class imbalance\n",
    "    if(class_imbal == True): \n",
    "        weight_pos = sum(y==0)/sum(y==1)\n",
    "    else: \n",
    "        weight_pos = 1\n",
    "    \n",
    "    #Fitting LightGBM with GOSS\n",
    "    if(subsample_type == 'goss'):\n",
    "        clf_lgb = lgb.LGBMClassifier(boosting_type = subsample_type,\n",
    "                                     learning_rate = learning_rate,\n",
    "                                     max_depth = max_depth,\n",
    "                                     feature_fraction = colsample,\n",
    "                                     num_leaves = 1100,\n",
    "                                     scale_pos_weight = weight_pos,\n",
    "                                     objective = 'binary',\n",
    "                                     enable_bundle = efb_enable,\n",
    "                                     top_rate = param_vals[0],\n",
    "                                     other_rate = param_vals[1],\n",
    "                                     n_estimators = int(param_vals[2]),\n",
    "                                     n_jobs = 1,\n",
    "                                     verbose = -1,\n",
    "                                     seed = 79)\n",
    "     \n",
    "    #Fitting LightGBM with standard row subsampling\n",
    "    elif(subsample_type == 'gbdt'):\n",
    "        clf_lgb = lgb.LGBMClassifier(boosting_type = subsample_type,\n",
    "                                     learning_rate = learning_rate,\n",
    "                                     max_depth = max_depth,\n",
    "                                     feature_fraction = colsample,\n",
    "                                     num_leaves = 1100,\n",
    "                                     scale_pos_weight = weight_pos,\n",
    "                                     objective = 'binary',\n",
    "                                     enable_bundle = efb_enable,\n",
    "                                     bagging_fraction = param_vals[0],\n",
    "                                     bagging_freq = 1,\n",
    "                                     n_estimators = int(param_vals[1]),\n",
    "                                     n_jobs = 1,\n",
    "                                     verbose = -1,\n",
    "                                     seed = 79)\n",
    "        \n",
    "    else: \n",
    "        raise ValueError(\"Must give subsampling type\")\n",
    "    \n",
    "    clf_lgb.fit(X, y)\n",
    "    \n",
    "    return(clf_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cff1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculating accuracy, F1, MCC and AUC of model\n",
    "def errors_model(mod, X_test, y_test):\n",
    "    \n",
    "    pred_class = mod.predict(X_test) #Class predictions\n",
    "    acc = accuracy_score(y_test, pred_class) #Accuracy\n",
    "    f1 = f1_score(y_test, pred_class) #F1 score \n",
    "    matt = matthews_corrcoef(y_test, pred_class) #Matthews Correlation Coefficient\n",
    "    pred_prob = mod.predict_proba(X_test)[:,1] #Probability predictions\n",
    "    auc = roc_auc_score(y_test, pred_prob) #AUC\n",
    "    errs_mod = [acc, f1, matt, auc]\n",
    "    return(errs_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7daf963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the test erros of the selected LightGBM models from cross-validation\n",
    "def test_errs(cv_sub, cv_goss, learning_rate, colsample, max_depth, class_imbal, efb_enable, X_train, y_train, X_test, y_test,cv_metric):\n",
    "    \n",
    "    lgbmod_test_err = {}\n",
    "    for i,x in enumerate(cv_goss):\n",
    "    \n",
    "        if(cv_metric=='AUC'):\n",
    "            ind_opt = np.argmax(cv_goss[x][:,3])\n",
    "            cv_goss_opt = cv_goss[x][ind_opt,:]\n",
    "        else:\n",
    "            ind_opt = np.argmin(cv_goss[x][:,3])\n",
    "            cv_goss_opt = cv_goss[x][ind_opt,:]\n",
    "            \n",
    "        cv_sub_ent = cv_sub[i,]\n",
    "    \n",
    "        params_fit = {\n",
    "            'cv_sub' : [list(cv_sub_ent),'gbdt'],\n",
    "            'cv_goss' : [list(cv_goss_opt),'goss']\n",
    "        }  \n",
    "    \n",
    "        errs_lgb = [[],[]]\n",
    "        for j, y in enumerate(params_fit):\n",
    "        \n",
    "            lgb_mod = lightgbm_fit_class(param_vals = params_fit[y][0], \n",
    "                                         subsample_type = params_fit[y][1], \n",
    "                                         X = X_train, \n",
    "                                         y = y_train, \n",
    "                                         learning_rate = learning_rate, \n",
    "                                         colsample = colsample, \n",
    "                                         max_depth = max_depth, \n",
    "                                         class_imbal = class_imbal, \n",
    "                                         efb_enable = efb_enable)\n",
    "        \n",
    "        \n",
    "            errs_lgb[j] = errors_model(mod = lgb_mod, X_test = X_test, y_test = y_test)\n",
    "    \n",
    "        lgbmod_test_err[x] = np.array(errs_lgb)\n",
    "        \n",
    "    return(lgbmod_test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807a4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting parameter values for row subsampling\n",
    "param_sub = np.round(np.linspace(0.2,0.8,num=7),1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57893916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting parameter values for GOSS\n",
    "param_goss = {}\n",
    "x = np.round(np.linspace(0.2,0.8,num=7),1)\n",
    "\n",
    "for sub_rat in x:\n",
    "    my_len = int(np.round(sub_rat/0.05))\n",
    "    \n",
    "    rat_comb = []\n",
    "    for i in range(1,my_len):\n",
    "        rat_comb.append(i*0.05)\n",
    "\n",
    "    rat_comb = np.round(rat_comb,4)\n",
    "    rat_comb = np.vstack((rat_comb,rat_comb[::-1]))\n",
    "    \n",
    "    param_goss[sub_rat] = rat_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3948c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No scientific notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f5349",
   "metadata": {},
   "source": [
    "# AdaBoost (Overlap data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d76ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset and setting up data \n",
    "import pickle\n",
    "file_name = 'Overlap_data.pickle'\n",
    "file = open(file_name,'rb')\n",
    "Overlap_data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train_over = Overlap_data['X_train']\n",
    "y_train_over = Overlap_data['y_train']\n",
    "X_test_over = Overlap_data['X_test']\n",
    "y_test_over = Overlap_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation with standard row subsampling\n",
    "cv_sub_over = lightgbm_tune_class(param_comb = param_sub, \n",
    "                                  subsample_type = 'gbdt', \n",
    "                                  X = X_train_over, \n",
    "                                  y = y_train_over, \n",
    "                                  learning_rate = 0.05, \n",
    "                                  num_iterations = 10000, \n",
    "                                  colsample = 1, \n",
    "                                  max_depth = 1,\n",
    "                                  class_imbal = False, \n",
    "                                  efb_enable = False,\n",
    "                                  cv_metric = 'binary_logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df50af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation with GOSS\n",
    "cv_goss_over = {}\n",
    "for x in param_goss:\n",
    "    cv_goss_over[x] = lightgbm_tune_class(param_comb = param_goss[x].T, \n",
    "                                          subsample_type = 'goss', \n",
    "                                          X = X_train_over, \n",
    "                                          y = y_train_over, \n",
    "                                          learning_rate = 0.05, \n",
    "                                          num_iterations = 10000, \n",
    "                                          colsample = 1, \n",
    "                                          max_depth = 1,\n",
    "                                          class_imbal = False, \n",
    "                                          efb_enable = False,\n",
    "                                          cv_metric = 'binary_logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ebb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting models, and calculating Accuracy, F1, MCC and AUC on test data\n",
    "test_errs_over = test_errs(cv_sub = cv_sub_over,\n",
    "                           cv_goss = cv_goss_over,\n",
    "                           learning_rate = 0.05, \n",
    "                           colsample = 1, \n",
    "                           max_depth = 1, \n",
    "                           class_imbal = False, \n",
    "                           efb_enable = False, \n",
    "                           X_train = X_train_over, \n",
    "                           y_train = y_train_over, \n",
    "                           X_test = X_test_over, \n",
    "                           y_test = y_test_over,\n",
    "                           cv_metric = 'binary_logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9b886",
   "metadata": {},
   "source": [
    "# Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7344c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and setting up data\n",
    "file_path_ph = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\phoneme.csv'\n",
    "df_ph = pd.read_csv(file_path_ph)\n",
    "X_ph = df_ph.drop('target',axis=1).copy()\n",
    "y_ph = df_ph['target'].copy()\n",
    "X_train_ph, X_test_ph, y_train_ph, y_test_ph = train_test_split(X_ph, y_ph, random_state=65, stratify=y_ph, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39016011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation with standard row subsampling\n",
    "cv_sub_ph = lightgbm_tune_class(param_comb = param_sub, \n",
    "                                subsample_type = 'gbdt', \n",
    "                                X = X_train_ph, \n",
    "                                y = y_train_ph, \n",
    "                                learning_rate = 0.05, \n",
    "                                num_iterations = 10000, \n",
    "                                colsample = 1, \n",
    "                                max_depth = 10,\n",
    "                                class_imbal = True, \n",
    "                                efb_enable = False,\n",
    "                                cv_metric = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d05712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation with GOSS\n",
    "cv_goss_ph = {}\n",
    "for x in param_goss:\n",
    "    cv_goss_ph[x] = lightgbm_tune_class(param_comb = param_goss[x].T, \n",
    "                                        subsample_type = 'goss', \n",
    "                                        X = X_train_ph, \n",
    "                                        y = y_train_ph, \n",
    "                                        learning_rate = 0.05, \n",
    "                                        num_iterations = 10000, \n",
    "                                        colsample = 1, \n",
    "                                        max_depth = 10,\n",
    "                                        class_imbal = True, \n",
    "                                        efb_enable = False,\n",
    "                                        cv_metric = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting models, and calculating Accuracy, F1, MCC and AUC on test data\n",
    "test_errs_ph = test_errs(cv_sub = cv_sub_ph,\n",
    "                         cv_goss = cv_goss_ph,\n",
    "                         learning_rate = 0.05, \n",
    "                         colsample = 1, \n",
    "                         max_depth = 10, \n",
    "                         class_imbal = True, \n",
    "                         efb_enable = False, \n",
    "                         X_train = X_train_ph, \n",
    "                         y_train = y_train_ph, \n",
    "                         X_test = X_test_ph, \n",
    "                         y_test = y_test_ph,\n",
    "                         cv_metric = 'AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bfaed6",
   "metadata": {},
   "source": [
    "# Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc82576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and setting up data.\n",
    "file_path = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\adult1.csv'\n",
    "df_adult1 = pd.read_csv(file_path,header=None)\n",
    "file_path = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\adult2.csv'\n",
    "df_adult2 = pd.read_csv(file_path,header=None)\n",
    "df_adult = pd.concat([df_adult1, df_adult2])\n",
    "df_adult.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"martial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"target\"]\n",
    "df_adult.replace(' ','',regex=True,inplace=True)\n",
    "df_adult['target'] = df_adult['target'].apply(lambda x: x.rstrip('.'))\n",
    "df_adult['target'] = df_adult['target'].apply(lambda x: 0 if x==\"<=50K\" else 1)\n",
    "df_adult = df_adult.reset_index(drop=True)\n",
    "df_adult = df_adult.drop('education',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2fe3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the categorical features\n",
    "ind_cat = df_adult.dtypes == object\n",
    "cat_feat = df_adult.columns[ind_cat].tolist()\n",
    "df_adult[cat_feat] = df_adult[cat_feat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc60a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training/Test split\n",
    "X_adult = df_adult.drop('target', axis=1)\n",
    "y_adult = df_adult['target']\n",
    "X_train_adult, X_test_adult, y_train_adult, y_test_adult = train_test_split(X_adult, y_adult, random_state=65, stratify=y_adult, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4704c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation with standard row subsampling\n",
    "cv_sub_adult = lightgbm_tune_class(param_comb = param_sub, \n",
    "                                   subsample_type = 'gbdt', \n",
    "                                   X = X_train_adult, \n",
    "                                   y = y_train_adult, \n",
    "                                   learning_rate = 0.05, \n",
    "                                   num_iterations = 10000, \n",
    "                                   colsample = 1,\n",
    "                                   max_depth = 8,\n",
    "                                   class_imbal = True, \n",
    "                                   efb_enable = False,\n",
    "                                   cv_metric = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60454817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation with GOSS\n",
    "cv_goss_adult = {}\n",
    "for x in param_goss:\n",
    "    cv_goss_adult[x] = lightgbm_tune_class(param_comb = param_goss[x].T, \n",
    "                                           subsample_type = 'goss', \n",
    "                                           X = X_train_adult, \n",
    "                                           y = y_train_adult, \n",
    "                                           learning_rate = 0.05, \n",
    "                                           num_iterations = 10000, \n",
    "                                           colsample = 1, \n",
    "                                           max_depth = 8,\n",
    "                                           class_imbal = True, \n",
    "                                           efb_enable = False, \n",
    "                                           cv_metric = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df43c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Accuracy, F1, MCC and AUC on test data\n",
    "test_errs_adult = test_errs(cv_sub = cv_sub_adult,\n",
    "          cv_goss = cv_goss_adult,\n",
    "          learning_rate = 0.05, \n",
    "          colsample = 1, \n",
    "          max_depth = 8, \n",
    "          class_imbal = True, \n",
    "          efb_enable = False, \n",
    "          X_train = X_train_adult, \n",
    "          y_train = y_train_adult, \n",
    "          X_test = X_test_adult, \n",
    "          y_test = y_test_adult, \n",
    "          cv_metric = 'AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0c60e",
   "metadata": {},
   "source": [
    "# Santander Customer Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "606f4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "import pickle\n",
    "file_name = 'customersatformat.pickle'\n",
    "file = open(file_name,'rb')\n",
    "df_sat = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8832e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training/Test split\n",
    "X_sat = df_sat.drop('TARGET', axis = 1).copy()\n",
    "y_sat = df_sat['TARGET'].copy()\n",
    "X_train_sat, X_test_sat, y_train_sat, y_test_sat = train_test_split(X_sat, y_sat, random_state=71, stratify=y_sat, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4f63e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation with standard row subsampling\n",
    "cv_sub_sat = lightgbm_tune_class(param_comb = param_sub, \n",
    "                                 subsample_type = 'gbdt', \n",
    "                                 X = X_train_sat, \n",
    "                                 y = y_train_sat, \n",
    "                                 learning_rate = 0.05, \n",
    "                                 num_iterations = 10000, \n",
    "                                 colsample = 0.3, \n",
    "                                 max_depth = 2,\n",
    "                                 class_imbal = True, \n",
    "                                 efb_enable = True,\n",
    "                                 cv_metric = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f9665d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation with GOSS\n",
    "cv_goss_sat = {}\n",
    "for x in param_goss:\n",
    "    cv_goss_sat[x] = lightgbm_tune_class(param_comb = param_goss[x].T, \n",
    "                                         subsample_type = 'goss', \n",
    "                                         X = X_train_sat, \n",
    "                                         y = y_train_sat, \n",
    "                                         learning_rate = 0.05, \n",
    "                                         num_iterations = 10000, \n",
    "                                         colsample = 0.3, \n",
    "                                         max_depth = 2,\n",
    "                                         class_imbal = True, \n",
    "                                         efb_enable = True, \n",
    "                                         cv_metric = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Accuracy, F1, MCC and AUC on test data\n",
    "test_errs_sat = test_errs(cv_sub = cv_sub_sat,\n",
    "          cv_goss = cv_goss_sat,\n",
    "          learning_rate = 0.05, \n",
    "          colsample = 0.3, \n",
    "          max_depth = 2, \n",
    "          class_imbal = True, \n",
    "          efb_enable = True, \n",
    "          X_train = X_train_sat, \n",
    "          y_train = y_train_sat, \n",
    "          X_test = X_test_sat, \n",
    "          y_test = y_test_sat, \n",
    "          cv_metric = 'AUC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
