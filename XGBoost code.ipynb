{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bb43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, plot_confusion_matrix, f1_score, accuracy_score, matthews_corrcoef\n",
    "from xgboost import cv\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560a82a",
   "metadata": {},
   "source": [
    "# XGBoost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e262db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform cross-validation with XGBoost for a grid of parameters.\n",
    "def xgboost_tune_class(param_comb, X, y, num_iterations, row_subsample, colsample, tree_build, class_imbal, sparse, learning_rate, cv_metric):\n",
    "\n",
    "    param_comb = np.hstack((param_comb, np.zeros((param_comb.shape[0], 2))))\n",
    "\n",
    "    #Checking if sparse matrix. If so, then 0 entries are treated as missing.\n",
    "    if(sparse == True): \n",
    "        xgb_dat = xgb.DMatrix(data=X,label=y,missing=0)\n",
    "    else: \n",
    "        xgb_dat = xgb.DMatrix(data=X,label=y)\n",
    "    \n",
    "    #Checking if class imbalance\n",
    "    if(class_imbal == True): \n",
    "        weight_pos = sum(y==0)/sum(y==1)\n",
    "    else: \n",
    "        weight_pos = 1\n",
    "\n",
    "    for i in range(param_comb.shape[0]):\n",
    "  \n",
    "        #Setting up parameters.\n",
    "        xgb_params = {\n",
    "            'eta' : learning_rate,\n",
    "            'gamma' : param_comb[i, 0],\n",
    "            'max_depth' : int(param_comb[i, 1]),\n",
    "            'subsample' : row_subsample,\n",
    "            'colsample_bytree' : colsample,\n",
    "            'reg_alpha': param_comb[i, 2],\n",
    "            'reg_lambda' : param_comb[i, 3],\n",
    "            'tree_method' : tree_build,\n",
    "            'scale_pos_weight' : weight_pos,\n",
    "            'objective' : 'binary:logistic',\n",
    "            'n_jobs' : 1\n",
    "        }\n",
    "\n",
    "        #Performing cross-validation\n",
    "        xgb_cv = xgb.cv(params = xgb_params,\n",
    "                        dtrain = xgb_dat,\n",
    "                        num_boost_round = num_iterations,\n",
    "                        nfold = 5,\n",
    "                        stratified = True,\n",
    "                        metrics = cv_metric,\n",
    "                        early_stopping_rounds = 200,\n",
    "                        seed = 75,\n",
    "                        verbose_eval = 100)\n",
    "    \n",
    "        if(cv_metric == 'auc'):\n",
    "            num_tree = np.argmax(xgb_cv.iloc[:,2]) + 1 #number of trees\n",
    "            opt_val = max(xgb_cv.iloc[:,2])\n",
    "        else:\n",
    "            num_tree = np.argmin(xgb_cv.iloc[:,2]) + 1 #number of trees\n",
    "            opt_val = min(xgb_cv.iloc[:,2])\n",
    "\n",
    "        param_comb[i,4:6] = num_tree, opt_val\n",
    "        \n",
    "        print(i)\n",
    "    \n",
    "    return(param_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1271c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to fit an XGBoost model with the optimal set of parameter values determined from cross-validation.\n",
    "def xgboost_fit_class(param_opt, X, y, row_subsample, colsample, tree_build, class_imbal, sparse, learning_rate):\n",
    "\n",
    "    #Checking if sparse matrix. If so, then 0 entries are treated as missing.\n",
    "    if(sparse == True): \n",
    "        missing_val = 0\n",
    "    else: \n",
    "        missing_val = np.nan\n",
    "    \n",
    "    #Checking if user specified class imbalance.\n",
    "    if(class_imbal == True): \n",
    "        weight_pos = sum(y==0)/sum(y==1)\n",
    "    else: \n",
    "        weight_pos = 1\n",
    "    \n",
    "    clf_xgb = xgb.XGBClassifier(eta = learning_rate,\n",
    "                                gamma = param_opt[0],\n",
    "                                max_depth = int(param_opt[1]),\n",
    "                                subsample = row_subsample,\n",
    "                                colsample_bytree = colsample,\n",
    "                                reg_alpha = param_opt[2],\n",
    "                                reg_lambda = param_opt[3],\n",
    "                                tree_method = tree_build,\n",
    "                                scale_pos_weight = weight_pos,\n",
    "                                objective = 'binary:logistic',\n",
    "                                seed = 33,\n",
    "                                n_estimators = int(param_opt[4]),\n",
    "                                missing = missing_val,\n",
    "                                verbosity = 1,\n",
    "                                n_jobs = 1)\n",
    "    #Fitting model\n",
    "    clf_xgb.fit(X, y)\n",
    "    \n",
    "    return(clf_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1fb4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove all the no regurlisation parameters from the parameter grid.\n",
    "def regularisation_param(gamma, max_depth, l1, l2):\n",
    "\n",
    "    param_comb_reg = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "    param_comb_reg = np.array(param_comb_reg)\n",
    "\n",
    "    #Removing no regularisation in grid (where gamma == 0 & l1 == 0 & l2 == 0)\n",
    "    zero_gamma = param_comb_reg[:,0] == 0\n",
    "    zero_alpha = param_comb_reg[:,2] == 0\n",
    "    zero_lambda = param_comb_reg[:,3] == 0\n",
    "    param_comb_reg = param_comb_reg[~zero_gamma | ~zero_alpha | ~zero_lambda,:]\n",
    "    \n",
    "    return(np.array(param_comb_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6109ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the errors of the models.\n",
    "def errors_model(mod, X_test, y_test):\n",
    "    \n",
    "    pred_class = mod.predict(X_test) #Class predictions\n",
    "    acc = accuracy_score(y_test, pred_class) #Accuracy\n",
    "    f1 = f1_score(y_test, pred_class) #F1 score \n",
    "    matt = matthews_corrcoef(y_test, pred_class) #Matthews Correlation Coefficient\n",
    "    pred_prob = mod.predict_proba(X_test)[:,1] #Probability predictions\n",
    "    auc = roc_auc_score(y_test, pred_prob) #AUC\n",
    "    errs_mod = [acc, f1, matt, auc]\n",
    "    return(errs_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19ff8a",
   "metadata": {},
   "source": [
    "# AdaBoost (Overlap data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a50eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset and setting up data \n",
    "import pickle\n",
    "file_name = 'Overlap_data.pickle'\n",
    "file = open(file_name,'rb')\n",
    "Overlap_data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train_over = Overlap_data['X_train']\n",
    "y_train_over = Overlap_data['y_train']\n",
    "X_test_over = Overlap_data['X_test']\n",
    "y_test_over = Overlap_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af375b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid no regularisation.\n",
    "gamma = [0]\n",
    "max_depth = list(range(1, 11))\n",
    "l1 = [0]\n",
    "l2 = [0]\n",
    "param_noreg_over = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "param_noreg_over = np.array(param_noreg_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d96da091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.69206+0.00007\ttest-logloss:0.69266+0.00015\n",
      "[100]\ttrain-logloss:0.63202+0.00182\ttest-logloss:0.65493+0.00437\n",
      "[200]\ttrain-logloss:0.59410+0.00276\ttest-logloss:0.62810+0.00620\n",
      "[300]\ttrain-logloss:0.56524+0.00330\ttest-logloss:0.60740+0.00816\n",
      "[400]\ttrain-logloss:0.54217+0.00365\ttest-logloss:0.59100+0.00966\n",
      "[500]\ttrain-logloss:0.52323+0.00393\ttest-logloss:0.57847+0.01093\n",
      "[600]\ttrain-logloss:0.50735+0.00418\ttest-logloss:0.56761+0.01168\n",
      "[700]\ttrain-logloss:0.49383+0.00441\ttest-logloss:0.55886+0.01268\n",
      "[800]\ttrain-logloss:0.48217+0.00464\ttest-logloss:0.55191+0.01344\n",
      "[900]\ttrain-logloss:0.47202+0.00485\ttest-logloss:0.54665+0.01401\n",
      "[1000]\ttrain-logloss:0.46312+0.00504\ttest-logloss:0.54175+0.01434\n",
      "[1100]\ttrain-logloss:0.45526+0.00521\ttest-logloss:0.53785+0.01508\n",
      "[1200]\ttrain-logloss:0.44826+0.00535\ttest-logloss:0.53470+0.01547\n",
      "[1300]\ttrain-logloss:0.44201+0.00548\ttest-logloss:0.53216+0.01602\n",
      "[1400]\ttrain-logloss:0.43641+0.00559\ttest-logloss:0.52983+0.01673\n",
      "[1500]\ttrain-logloss:0.43133+0.00568\ttest-logloss:0.52841+0.01721\n",
      "[1600]\ttrain-logloss:0.42675+0.00576\ttest-logloss:0.52722+0.01791\n",
      "[1700]\ttrain-logloss:0.42257+0.00582\ttest-logloss:0.52625+0.01835\n",
      "[1800]\ttrain-logloss:0.41875+0.00587\ttest-logloss:0.52568+0.01855\n",
      "[1900]\ttrain-logloss:0.41524+0.00590\ttest-logloss:0.52506+0.01909\n",
      "[2000]\ttrain-logloss:0.41200+0.00592\ttest-logloss:0.52488+0.01962\n",
      "[2100]\ttrain-logloss:0.40898+0.00594\ttest-logloss:0.52472+0.02003\n",
      "[2200]\ttrain-logloss:0.40618+0.00595\ttest-logloss:0.52443+0.02024\n",
      "[2300]\ttrain-logloss:0.40355+0.00597\ttest-logloss:0.52459+0.02070\n",
      "[2372]\ttrain-logloss:0.40176+0.00598\ttest-logloss:0.52469+0.02068\n",
      "0\n",
      "[0]\ttrain-logloss:0.69088+0.00012\ttest-logloss:0.69215+0.00054\n",
      "[100]\ttrain-logloss:0.57019+0.00333\ttest-logloss:0.62448+0.00795\n",
      "[200]\ttrain-logloss:0.50238+0.00411\ttest-logloss:0.58840+0.01013\n",
      "[300]\ttrain-logloss:0.45480+0.00420\ttest-logloss:0.56700+0.01122\n",
      "[400]\ttrain-logloss:0.41904+0.00458\ttest-logloss:0.55433+0.01314\n",
      "[500]\ttrain-logloss:0.39003+0.00466\ttest-logloss:0.54775+0.01415\n",
      "[600]\ttrain-logloss:0.36530+0.00477\ttest-logloss:0.54356+0.01472\n",
      "[700]\ttrain-logloss:0.34327+0.00525\ttest-logloss:0.54130+0.01589\n",
      "[800]\ttrain-logloss:0.32367+0.00567\ttest-logloss:0.54186+0.01761\n",
      "[900]\ttrain-logloss:0.30615+0.00592\ttest-logloss:0.54128+0.01967\n",
      "[958]\ttrain-logloss:0.29642+0.00608\ttest-logloss:0.54283+0.02094\n",
      "1\n",
      "[0]\ttrain-logloss:0.68949+0.00011\ttest-logloss:0.69169+0.00068\n",
      "[100]\ttrain-logloss:0.50033+0.00432\ttest-logloss:0.60257+0.00818\n",
      "[200]\ttrain-logloss:0.40554+0.00466\ttest-logloss:0.56789+0.01365\n",
      "[300]\ttrain-logloss:0.34397+0.00405\ttest-logloss:0.55394+0.01739\n",
      "[400]\ttrain-logloss:0.29795+0.00400\ttest-logloss:0.54967+0.01911\n",
      "[500]\ttrain-logloss:0.26103+0.00353\ttest-logloss:0.55106+0.02257\n",
      "[600]\ttrain-logloss:0.22933+0.00348\ttest-logloss:0.55347+0.02424\n",
      "[605]\ttrain-logloss:0.22792+0.00346\ttest-logloss:0.55359+0.02459\n",
      "2\n",
      "[0]\ttrain-logloss:0.68760+0.00023\ttest-logloss:0.69110+0.00056\n",
      "[100]\ttrain-logloss:0.42498+0.00472\ttest-logloss:0.58633+0.01241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9304\\4136303202.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Tuning model with no regularisation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m cv_noreg_over = xgboost_tune_class(param_comb = param_noreg_over, \n\u001b[0m\u001b[0;32m      3\u001b[0m                                    \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_over\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                    \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train_over\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                    \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9304\\623627711.py\u001b[0m in \u001b[0;36mxgboost_tune_class\u001b[1;34m(param_comb, X, y, num_iterations, row_subsample, colsample, tree_build, class_imbal, sparse, learning_rate, cv_metric)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#Performing cross-validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         xgb_cv = xgb.cv(params = xgb_params,\n\u001b[0m\u001b[0;32m     37\u001b[0m                         \u001b[0mdtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_dat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                         \u001b[0mnum_boost_round\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle, custom_metric)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[0mshould_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, iteration, obj)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;34m'''Iterate through folds for update'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvfolds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, iteration, fobj)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mObjective\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Tuning model with no regularisation.\n",
    "cv_noreg_over = xgboost_tune_class(param_comb = param_noreg_over, \n",
    "                                   X = X_train_over, \n",
    "                                   y = y_train_over, \n",
    "                                   learning_rate = 0.05, \n",
    "                                   num_iterations = 10000, \n",
    "                                   row_subsample = 1, \n",
    "                                   colsample = 1, \n",
    "                                   tree_build = 'exact', \n",
    "                                   class_imbal = False, \n",
    "                                   sparse = False,\n",
    "                                   cv_metric = 'logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2168e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with no regularisation.\n",
    "opt_ind_noreg_over = np.argmin(cv_noreg_over[:,5])\n",
    "xgmod_noreg_over = xgboost_fit_class(param_opt = cv_noreg_over[opt_ind_noreg_over,:], \n",
    "                                     X = X_train_over, \n",
    "                                     y = y_train_over, \n",
    "                                     learning_rate = 0.05,\n",
    "                                     row_subsample = 1, \n",
    "                                     colsample = 1, \n",
    "                                     tree_build = 'exact',\n",
    "                                     class_imbal = False,\n",
    "                                     sparse = False)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_noreg_over = errors_model(mod = xgmod_noreg_over, X_test = X_test_over, y_test = y_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59a038c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid regularisation.\n",
    "\n",
    "#Parameters 1\n",
    "###################################################################################\n",
    "gamma = [0, 0.5, 1, 2, 3]\n",
    "max_depth = [1, 2, 3]\n",
    "l1 = [0, 0.5, 1, 2, 3]\n",
    "l2 = [0, 0.5, 1, 2, 3]\n",
    "param_reg_over = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################\n",
    "\n",
    "#Parameters 2 (refined)\n",
    "###################################################################################\n",
    "#gamma = [0]\n",
    "#max_depth = [1]\n",
    "#l1 = [0.5]\n",
    "#l2 = [3, 3.5, 4, 4.5 ,5, 6]\n",
    "#param_reg_over = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a34054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with regularisation.\n",
    "cv_reg_over = xgboost_tune_class(param_comb = param_reg_over,\n",
    "                                 X = X_train_over, \n",
    "                                 y = y_train_over, \n",
    "                                 learning_rate = 0.05, \n",
    "                                 num_iterations = 10000, \n",
    "                                 row_subsample = 1, \n",
    "                                 colsample = 1, \n",
    "                                 tree_build = 'exact', \n",
    "                                 class_imbal = False, \n",
    "                                 sparse = False,\n",
    "                                 cv_metric = 'logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b435760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with regularisation.\n",
    "opt_ind_reg_over = np.argmin(cv_reg_over[:,5])\n",
    "xgmod_reg_over = xgboost_fit_class(param_opt = cv_reg_over[opt_ind_reg_over,:], \n",
    "                                     X = X_train_over, \n",
    "                                     y = y_train_over, \n",
    "                                     learning_rate = 0.05,\n",
    "                                     row_subsample = 1, \n",
    "                                     colsample = 1, \n",
    "                                     tree_build = 'exact',\n",
    "                                     class_imbal = False,\n",
    "                                     sparse = False)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_reg_over = errors_model(mod = xgmod_reg_over, X_test = X_test_over, y_test = y_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f1d2050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.   ,    1.   ,    0.   ,    0.   , 2173.   ,    0.756,\n",
       "           0.758,    0.511,    0.829],\n",
       "       [   0.   ,    1.   ,    0.5  ,    3.   , 3467.   ,    0.757,\n",
       "           0.76 ,    0.515,    0.828]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Table of parameters and test errors. \n",
    "test_err_noreg = np.append(cv_noreg_over[opt_ind_noreg_over,0:5],errs_noreg_over)\n",
    "test_err_reg = np.append(cv_reg_over[opt_ind_reg_over,0:5],errs_reg_over)\n",
    "test_err_over = np.row_stack((test_err_noreg,test_err_reg))\n",
    "np.round(test_err_over,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec818e",
   "metadata": {},
   "source": [
    "# Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7124c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and setting up data.\n",
    "file_path_ph = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\phoneme.csv'\n",
    "df_ph = pd.read_csv(file_path_ph)\n",
    "X_ph = df_ph.drop('target',axis=1).copy()\n",
    "y_ph = df_ph['target'].copy()\n",
    "X_train_ph, X_test_ph, y_train_ph, y_test_ph = train_test_split(X_ph, y_ph, random_state=65, stratify=y_ph, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6aa797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid no regularisation.\n",
    "gamma = [0]\n",
    "max_depth = list(range(1, 11))\n",
    "l1 = [0]\n",
    "l2 = [0]\n",
    "param_noreg_ph = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "param_noreg_ph = np.array(param_noreg_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1db08288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with no regularisation.\n",
    "cv_noreg_ph = xgboost_tune_class(param_comb = param_noreg_ph, \n",
    "                                 X = X_train_ph, \n",
    "                                 y = y_train_ph, \n",
    "                                 learning_rate = 0.05, \n",
    "                                 num_iterations = 10000, \n",
    "                                 row_subsample = 0.8, \n",
    "                                 colsample = 1, \n",
    "                                 tree_build = 'exact', \n",
    "                                 class_imbal = True, \n",
    "                                 sparse = False,\n",
    "                                 cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa19e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with no regularisation.\n",
    "opt_ind_noreg_ph = np.argmax(cv_noreg_ph[:,5])\n",
    "xgmod_noreg_ph = xgboost_fit_class(param_opt = cv_noreg_ph[opt_ind_noreg_ph,:], \n",
    "                                   X = X_train_ph, \n",
    "                                   y = y_train_ph, \n",
    "                                   learning_rate = 0.05,\n",
    "                                   row_subsample = 0.8, \n",
    "                                   colsample = 1, \n",
    "                                   tree_build = 'exact',\n",
    "                                   class_imbal = True,\n",
    "                                   sparse = False)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_noreg_ph = errors_model(mod = xgmod_noreg_ph, X_test = X_test_ph, y_test = y_test_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68040b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid regularisation.\n",
    "\n",
    "#Parameters 1\n",
    "#####################################################################\n",
    "gamma = [0, 0.5, 1, 2, 3]\n",
    "max_depth = [10]\n",
    "l1 = [0, 0.5, 1, 2, 3]\n",
    "l2 = [0, 0.5, 1, 2, 3]\n",
    "param_reg_ph = regularisation_param(gamma, max_depth, l1, l2)\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d29a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with regularisation.\n",
    "cv_reg_ph = xgboost_tune_class(param_comb = param_reg_ph,\n",
    "                               X = X_train_ph, \n",
    "                               y = y_train_ph, \n",
    "                               learning_rate = 0.05, \n",
    "                               num_iterations = 10000, \n",
    "                               row_subsample = 0.8, \n",
    "                               colsample = 1, \n",
    "                               tree_build = 'exact', \n",
    "                               class_imbal = True, \n",
    "                               sparse = False,\n",
    "                               cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b381d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with regularisation.\n",
    "opt_ind_reg_ph = np.argmax(cv_reg_ph[:,5])\n",
    "xgmod_reg_ph = xgboost_fit_class(param_opt = cv_reg_ph[opt_ind_reg_ph,:], \n",
    "                                 X = X_train_ph, \n",
    "                                 y = y_train_ph, \n",
    "                                 learning_rate = 0.05,\n",
    "                                 row_subsample = 0.8, \n",
    "                                 colsample = 1, \n",
    "                                 tree_build = 'exact',\n",
    "                                 class_imbal = True,\n",
    "                                 sparse = False)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_reg_ph = errors_model(mod = xgmod_reg_ph, X_test = X_test_ph, y_test = y_test_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f015233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.   ,  10.   ,   0.   ,   0.   , 298.   ,   0.907,   0.846,\n",
       "          0.781,   0.963],\n",
       "       [  0.   ,  10.   ,   0.   ,   2.   , 574.   ,   0.907,   0.845,\n",
       "          0.779,   0.964]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Table of parameters and test errors. \n",
    "test_err_noreg = np.append(cv_noreg_ph[opt_ind_noreg_ph,0:5],errs_noreg_ph)\n",
    "test_err_reg = np.append(cv_reg_ph[opt_ind_reg_ph,0:5],errs_reg_ph)\n",
    "test_err_ph = np.row_stack((test_err_noreg,test_err_reg))\n",
    "np.round(test_err_ph,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca7bce",
   "metadata": {},
   "source": [
    "# Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1bcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and setting up data.\n",
    "file_path = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\adult1.csv'\n",
    "df_adult1 = pd.read_csv(file_path,header=None)\n",
    "file_path = 'C:\\\\Users\\\\Matt\\\\Documents\\\\Python code thesis\\\\Datasets\\\\adult2.csv'\n",
    "df_adult2 = pd.read_csv(file_path,header=None)\n",
    "df_adult = pd.concat([df_adult1, df_adult2])\n",
    "df_adult.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"martial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"target\"]\n",
    "df_adult.replace(' ','',regex=True,inplace=True)\n",
    "df_adult['target'] = df_adult['target'].apply(lambda x: x.rstrip('.'))\n",
    "df_adult['target'] = df_adult['target'].apply(lambda x: 0 if x==\"<=50K\" else 1)\n",
    "df_adult = df_adult.reset_index(drop=True)\n",
    "df_adult = df_adult.drop('education',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad39e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dummy varaibles and Training/Test split\n",
    "X_adult = df_adult.drop('target', axis = 1).copy()\n",
    "X_adult = pd.get_dummies(X_adult, columns = X_adult.columns[X_adult.dtypes==object], drop_first=True)\n",
    "y_adult = df_adult['target'].copy()\n",
    "X_train_adult, X_test_adult, y_train_adult, y_test_adult = train_test_split(X_adult, y_adult, random_state=65, stratify=y_adult, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceff603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid no regularisation.\n",
    "gamma = [0]\n",
    "max_depth = list(range(1, 11))\n",
    "l1 = [0]\n",
    "l2 = [0]\n",
    "param_noreg_adult = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "param_noreg_adult = np.array(param_noreg_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe45308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with no regularisation.\n",
    "cv_noreg_adult = xgboost_tune_class(param_comb = param_noreg_adult, \n",
    "                                    X = X_train_adult, \n",
    "                                    y = y_train_adult, \n",
    "                                    learning_rate = 0.05, \n",
    "                                    num_iterations = 10000, \n",
    "                                    row_subsample = 0.5, \n",
    "                                    colsample = 0.5, \n",
    "                                    tree_build = 'approx',\n",
    "                                    class_imbal = True,\n",
    "                                    sparse = True,\n",
    "                                    cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee60d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with no regularisation.\n",
    "opt_ind_noreg_adult = np.argmax(cv_noreg_adult[:,5])\n",
    "xgmod_noreg_adult = xgboost_fit_class(param_opt = cv_noreg_adult[opt_ind_noreg_adult,:], \n",
    "                                   X = X_train_adult, \n",
    "                                   y = y_train_adult, \n",
    "                                   learning_rate = 0.05,\n",
    "                                   row_subsample = 0.5, \n",
    "                                   colsample = 0.5, \n",
    "                                   tree_build = 'approx',\n",
    "                                   class_imbal = True,\n",
    "                                   sparse = True)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_noreg_adult = errors_model(mod = xgmod_noreg_adult, X_test = X_test_adult, y_test = y_test_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c4b9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with regularisation.\n",
    "cv_reg_adult = xgboost_tune_class(param_comb = param_reg_adult,\n",
    "                                  X = X_train_adult, \n",
    "                                  y = y_train_adult, \n",
    "                                  learning_rate = 0.05, \n",
    "                                  num_iterations = 10000, \n",
    "                                  row_subsample = 0.5, \n",
    "                                  colsample = 0.5, \n",
    "                                  tree_build = 'approx',\n",
    "                                  class_imbal = True,\n",
    "                                  sparse = True,\n",
    "                                  cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d09069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with regularisation.\n",
    "opt_ind_reg_adult = np.argmax(cv_reg_adult[:,5])\n",
    "xgmod_reg_adult = xgboost_fit_class(param_opt = cv_reg_adult[opt_ind_reg_adult,:], \n",
    "                                   X = X_train_adult, \n",
    "                                   y = y_train_adult, \n",
    "                                   learning_rate = 0.05,\n",
    "                                   row_subsample = 0.5, \n",
    "                                   colsample = 0.5, \n",
    "                                   tree_build = 'approx',\n",
    "                                   class_imbal = True,\n",
    "                                   sparse = True)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_reg_adult = errors_model(mod = xgmod_reg_adult, X_test = X_test_adult, y_test = y_test_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48dfeead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.    ,   8.    ,   0.    ,   0.    , 211.    ,   0.8398,\n",
       "          0.7223,   0.6322,   0.9319],\n",
       "       [  0.    ,   8.    ,   0.    ,   0.5   , 219.    ,   0.8409,\n",
       "          0.7234,   0.6335,   0.932 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Table of parameters and test errors. \n",
    "test_err_noreg = np.append(cv_noreg_adult[opt_ind_noreg_adult,0:5],errs_noreg_adult)\n",
    "test_err_reg = np.append(cv_reg_adult[opt_ind_reg_adult,0:5],errs_reg_adult)\n",
    "test_err_adult = np.row_stack((test_err_noreg,test_err_reg))\n",
    "np.round(test_err_adult,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09de9de",
   "metadata": {},
   "source": [
    "# Santander Customer Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7011368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset and setting up data.\n",
    "import pickle\n",
    "file_name = 'customersatformat.pickle'\n",
    "file = open(file_name,'rb')\n",
    "df_sat = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_sat = df_sat.drop('TARGET', axis = 1).copy()\n",
    "y_sat = df_sat['TARGET'].copy()\n",
    "X_train_sat, X_test_sat, y_train_sat, y_test_sat = train_test_split(X_sat, y_sat, random_state=71, stratify=y_sat, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00bad5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid no regularisation.\n",
    "gamma = [0]\n",
    "max_depth = list(range(1, 11))\n",
    "l1 = [0]\n",
    "l2 = [0]\n",
    "param_noreg_sat = list(itertools.product(gamma, max_depth, l1, l2))\n",
    "param_noreg_sat = np.array(param_noreg_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f63fa8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.68308+0.00224\ttest-auc:0.68082+0.00574\n",
      "[100]\ttrain-auc:0.82846+0.00124\ttest-auc:0.82471+0.00454\n",
      "[200]\ttrain-auc:0.83746+0.00076\ttest-auc:0.83213+0.00476\n",
      "[300]\ttrain-auc:0.84183+0.00101\ttest-auc:0.83507+0.00470\n",
      "[400]\ttrain-auc:0.84452+0.00107\ttest-auc:0.83574+0.00507\n",
      "[500]\ttrain-auc:0.84659+0.00101\ttest-auc:0.83616+0.00491\n",
      "[600]\ttrain-auc:0.84821+0.00107\ttest-auc:0.83583+0.00512\n",
      "[664]\ttrain-auc:0.84919+0.00109\ttest-auc:0.83578+0.00550\n",
      "0\n",
      "[0]\ttrain-auc:0.77508+0.00302\ttest-auc:0.77210+0.00404\n",
      "[100]\ttrain-auc:0.84439+0.00071\ttest-auc:0.83635+0.00360\n",
      "[200]\ttrain-auc:0.85301+0.00042\ttest-auc:0.83949+0.00411\n",
      "[300]\ttrain-auc:0.85858+0.00060\ttest-auc:0.83922+0.00381\n",
      "[400]\ttrain-auc:0.86282+0.00031\ttest-auc:0.83852+0.00345\n",
      "[421]\ttrain-auc:0.86365+0.00045\ttest-auc:0.83861+0.00334\n",
      "1\n",
      "[0]\ttrain-auc:0.80671+0.00631\ttest-auc:0.80050+0.00529\n",
      "[100]\ttrain-auc:0.85481+0.00038\ttest-auc:0.83754+0.00481\n",
      "[200]\ttrain-auc:0.86679+0.00067\ttest-auc:0.83797+0.00564\n",
      "[300]\ttrain-auc:0.87500+0.00068\ttest-auc:0.83618+0.00548\n",
      "[351]\ttrain-auc:0.87836+0.00070\ttest-auc:0.83495+0.00574\n",
      "2\n",
      "[0]\ttrain-auc:0.81348+0.00743\ttest-auc:0.80380+0.00717\n",
      "[100]\ttrain-auc:0.86657+0.00042\ttest-auc:0.83540+0.00502\n",
      "[200]\ttrain-auc:0.88191+0.00087\ttest-auc:0.83382+0.00437\n",
      "[300]\ttrain-auc:0.89252+0.00098\ttest-auc:0.83020+0.00461\n",
      "[351]\ttrain-auc:0.89693+0.00112\ttest-auc:0.82793+0.00486\n",
      "3\n",
      "[0]\ttrain-auc:0.81356+0.00663\ttest-auc:0.79731+0.00639\n",
      "[100]\ttrain-auc:0.87818+0.00055\ttest-auc:0.83110+0.00492\n",
      "[200]\ttrain-auc:0.89668+0.00026\ttest-auc:0.82702+0.00480\n",
      "[279]\ttrain-auc:0.90695+0.00057\ttest-auc:0.82225+0.00399\n",
      "4\n",
      "[0]\ttrain-auc:0.81218+0.00338\ttest-auc:0.78594+0.00838\n",
      "[100]\ttrain-auc:0.88972+0.00064\ttest-auc:0.82538+0.00539\n",
      "[200]\ttrain-auc:0.91025+0.00050\ttest-auc:0.81912+0.00489\n",
      "[280]\ttrain-auc:0.92211+0.00037\ttest-auc:0.81324+0.00355\n",
      "5\n",
      "[0]\ttrain-auc:0.80296+0.00599\ttest-auc:0.76331+0.01789\n",
      "[100]\ttrain-auc:0.90103+0.00049\ttest-auc:0.81854+0.00428\n",
      "[200]\ttrain-auc:0.92338+0.00065\ttest-auc:0.80962+0.00423\n",
      "[268]\ttrain-auc:0.93443+0.00030\ttest-auc:0.80344+0.00281\n",
      "6\n",
      "[0]\ttrain-auc:0.79906+0.00838\ttest-auc:0.75307+0.01716\n",
      "[100]\ttrain-auc:0.91027+0.00065\ttest-auc:0.81084+0.00651\n",
      "[200]\ttrain-auc:0.93364+0.00044\ttest-auc:0.79826+0.00590\n",
      "[268]\ttrain-auc:0.94462+0.00049\ttest-auc:0.79190+0.00631\n",
      "7\n",
      "[0]\ttrain-auc:0.79378+0.00722\ttest-auc:0.74405+0.01344\n",
      "[100]\ttrain-auc:0.91960+0.00073\ttest-auc:0.80492+0.00618\n",
      "[200]\ttrain-auc:0.94264+0.00063\ttest-auc:0.79173+0.00562\n",
      "[280]\ttrain-auc:0.95493+0.00076\ttest-auc:0.78511+0.00635\n",
      "8\n",
      "[0]\ttrain-auc:0.79158+0.00860\ttest-auc:0.73359+0.01023\n",
      "[100]\ttrain-auc:0.92651+0.00091\ttest-auc:0.79668+0.00663\n",
      "[200]\ttrain-auc:0.94937+0.00118\ttest-auc:0.78237+0.00539\n",
      "[280]\ttrain-auc:0.96174+0.00112\ttest-auc:0.77722+0.00639\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#Tuning model with no regularisation.\n",
    "cv_noreg_sat = xgboost_tune_class(param_comb = param_noreg_sat, \n",
    "                                  X = X_train_sat, \n",
    "                                  y = y_train_sat, \n",
    "                                  learning_rate = 0.05, \n",
    "                                  num_iterations = 10000, \n",
    "                                  row_subsample = 0.3, \n",
    "                                  colsample = 0.3, \n",
    "                                  tree_build = 'approx', \n",
    "                                  class_imbal = True, \n",
    "                                  sparse = True,\n",
    "                                  cv_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34593fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with no regularisation.\n",
    "opt_ind_noreg_sat = np.argmax(cv_noreg_sat[:,5])\n",
    "xgmod_noreg_sat = xgboost_fit_class(param_opt = cv_noreg_sat[opt_ind_noreg_sat,:], \n",
    "                                   X = X_train_sat, \n",
    "                                   y = y_train_sat, \n",
    "                                   learning_rate = 0.05,\n",
    "                                   row_subsample = 0.3, \n",
    "                                   colsample = 0.3, \n",
    "                                   tree_build = 'approx', \n",
    "                                   class_imbal = True, \n",
    "                                   sparse = True)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_noreg_sat = errors_model(mod = xgmod_noreg_sat, X_test = X_test_sat, y_test = y_test_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27692a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid regularisation\n",
    "\n",
    "#Parameters 1\n",
    "###################################################################################\n",
    "gamma = [0, 0.5, 1, 2, 3]\n",
    "max_depth = [2, 3, 4]\n",
    "l1 = [0, 0.5, 1, 2, 3]\n",
    "l2 = [0, 0.5, 1, 2, 3]\n",
    "param_reg_sat = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################\n",
    "\n",
    "#Parameters 2\n",
    "###################################################################################\n",
    "#gamma = [2]\n",
    "#max_depth = [3]\n",
    "#l1 = [2]\n",
    "#l2 = [3, 4.5, 5, 5.5, 6]\n",
    "param_reg_sat = regularisation_param(gamma, max_depth, l1, l2)\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b81e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning model with regularisation.\n",
    "cv_reg_sat = xgboost_tune_class(param_comb = param_reg_sat,\n",
    "                                X = X_train_sat, \n",
    "                                y = y_train_sat, \n",
    "                                learning_rate = 0.05, \n",
    "                                num_iterations = 10000, \n",
    "                                row_subsample = 0.3, \n",
    "                                colsample = 0.3, \n",
    "                                tree_build = 'approx', \n",
    "                                class_imbal = True, \n",
    "                                sparse = True,\n",
    "                                cv_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902d64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting optimal XGBoost model with regularisation.\n",
    "opt_ind_reg_sat = np.argmax(cv_reg_sat[:,5])\n",
    "xgmod_reg_sat = xgboost_fit_class(param_opt = cv_reg_sat[opt_ind_reg_sat,:], \n",
    "                                  X = X_train_sat, \n",
    "                                  y = y_train_sat, \n",
    "                                  learning_rate = 0.05,\n",
    "                                  row_subsample = 0.3, \n",
    "                                  colsample = 0.3, \n",
    "                                  tree_build = 'approx', \n",
    "                                  class_imbal = True, \n",
    "                                  sparse = True)\n",
    "\n",
    "#Calculating errors of model\n",
    "errs_reg_sat = errors_model(mod = xgmod_reg_sat, X_test = X_test_sat, y_test = y_test_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b638967a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.    ,   2.    ,   0.    ,   0.    , 222.    ,   0.7708,\n",
       "          0.2031,   0.2306,   0.8317],\n",
       "       [  2.    ,   3.    ,   2.    ,   3.    , 175.    ,   0.7798,\n",
       "          0.2093,   0.2371,   0.8335]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Table of parameters and test errors. \n",
    "test_err_noreg = np.append(cv_noreg_sat[opt_ind_noreg_sat,0:5],errs_noreg_sat)\n",
    "test_err_reg = np.append(cv_reg_sat[opt_ind_reg_sat,0:5],errs_reg_sat)\n",
    "test_err_sat = np.row_stack((test_err_noreg,test_err_reg))\n",
    "np.round(test_err_sat,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
